{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work To Do\n",
    "\n",
    "1. load images\n",
    "2. resiz images\n",
    "3. def model\n",
    "4. load mdel\n",
    "5. def gram matrix\n",
    "6. noramlize images\n",
    "7. compute content loss, style loss\n",
    "8. compute total loss and backward\n",
    "9. save image(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the Image\n",
    "CONTENT_IMG = \"content.jpg\"\n",
    "STYLE_IMG = \"style.jpg\"\n",
    "\n",
    "STEPS = 500\n",
    "MODEL = 'vgg19'\n",
    "\n",
    "IMSIZE = 512 if torch.cuda.is_available() else 128\n",
    "\n",
    "MEAN=[0.485, 0.456, 0.406]\n",
    "STD=[0.229, 0.224, 0.225]\n",
    "\n",
    "CNTWEIGHT = 1\n",
    "STLWEIGHT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the Image\n",
    "    #1. pil->tensor\n",
    "convert = transforms.Compose([\n",
    "    transforms.Resize(IMSIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "'''이렇게만 하면, 같은 사이즈의 이미지만 처리할 수 있다. \n",
    "    더 확장하려면 mean 을 -,+ 과정을 거쳐야 함'''\n",
    "    # 2. tensor -> pil\n",
    "reconvert = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "#이미지 로드 함수\n",
    "def imageLoad(image_name):\n",
    "    img = Image.open(image_name)\n",
    "    img = convert(img).unsqueeze(0).clone()\n",
    "    return img.to(device, torch.float)\n",
    "\n",
    "def showImage(tensor):\n",
    "    image = tensor.cpu().clone() #not to change original image\n",
    "    image = image.squeeze(0) #delete b  \n",
    "    image = reconvert(image)\n",
    "    plt.imshow(image) #image show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gramMatrix(img):\n",
    "    b, c, h, w = img.size() #batch size(1), chanel, height, width\n",
    "    features = img.view(b*c, h*w) \n",
    "    gram = torch.mm(features, features.t())\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentLoss(targetList,content_layers_target, input_img_content):\n",
    "    loss = 0\n",
    "    for i in targetList:\n",
    "        loss += F.mse_loss(content_layers_target[0], input_img_content[0])/2.0\n",
    "        ##print(\"content\", loss, type(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styleLoss(targetList, style_layers_target, input_img_style, vgg):\n",
    "    loss = 0\n",
    "    j = 0\n",
    "    for i in targetList:\n",
    "        n = vgg[i].out_channels\n",
    "        m = vgg[i].kernel_size[0] * vgg[i].kernel_size[1]\n",
    "        loss +=F.mse_loss(gramMatrix(style_layers_target[j]), gramMatrix(input_img_style[j]))/(n*n*m*4)\n",
    "        j +=1\n",
    "        ##print(\"style\", loss, type(loss))\n",
    "    return loss * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNewImg(noise_img, vgg, content_layers_target, style_layers_target, input_img_content, input_img_style, content_layers,  style_layers):\n",
    "    run = [0]\n",
    "    while run[0] < STEPS:\n",
    "        \n",
    "        def closure():\n",
    "            vgg(noise_img)\n",
    "            optimizer.zero_grad()\n",
    "            content_loss = contentLoss(content_layers,content_layers_target, input_img_content)\n",
    "            style_loss = styleLoss(style_layers, style_layers_target, input_img_style, vgg)\n",
    "            total_loss = content_loss * CNTWEIGHT + style_loss * STLWEIGHT \n",
    "            total_loss = Variable(total_loss, requires_grad = True)\n",
    "            total_loss.backward()\n",
    "\n",
    "            if run[0] % 10 == 0:\n",
    "                print('Step - {}, Content loss - {}, Style loss - {}, Total Loss - {}'.format(run[0], content_loss, style_loss, total_loss));\n",
    "                ##new_img = out_img.data.cpu().squeeze().permute(1,2,0).numpy();\n",
    "                ##cv2.imwrite(os.path.join('result', str(count) + '.png'), new_img*255);\n",
    "            run[0] +=1\n",
    "            return content_loss + style_loss\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "    return noise_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "Step - 0, Content loss - 2.950841188430786, Style loss - 46863.17578125, Total Loss - 46863180.0\n",
      "Step - 10, Content loss - 2.950841188430786, Style loss - 46863.17578125, Total Loss - 46863180.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #cuda load\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #images load\n",
    "    content_img = imageLoad(CONTENT_IMG)\n",
    "    style_img = imageLoad(STYLE_IMG)    \n",
    "    plt.figure()\n",
    "    showImage(style_img)\n",
    "    plt.figure()\n",
    "    showImage(content_img)\n",
    "    \n",
    "    #model load\n",
    "    vgg = models.vgg19(pretrained=True).features.to(device)\n",
    "    \n",
    "    #white noise image\n",
    "    noise_img = torch.randn(content_img.data.size(), device=device)\n",
    "    \n",
    "    #normalize \n",
    "    normalization = transforms.Normalize(MEAN,STD)\n",
    "    \n",
    "    #initialize losses\n",
    "    content_layers = [3]#cov4_2\n",
    "    style_layers = [0, 5, 10, 19, 28]#conv1_1, conv2_1, conv3_1, conv4_1, conv5_1\n",
    "    content_layers_target = []\n",
    "    style_layers_target = []\n",
    "    input_img_content = []\n",
    "    input_img_style = []\n",
    "\n",
    "    for i in content_layers:\n",
    "        tmp_model = vgg[:(i+1)]\n",
    "        target = tmp_model(content_img).detach()\n",
    "        content_layers_target.append(target)\n",
    "        input_target = tmp_model(noise_img).detach()\n",
    "        input_img_content.append(input_target)\n",
    "    for i in style_layers:\n",
    "        tmp_model = vgg[:(i+1)]\n",
    "        target = tmp_model(style_img).detach()\n",
    "        style_layers_target.append(target)\n",
    "        input_target = tmp_model(noise_img).detach()\n",
    "        input_img_style.append(input_target)\n",
    "        \n",
    "    print(len(content_layers_target))\n",
    "    print(len(style_layers_target))\n",
    "    \n",
    "    #content_losses = [ContentLoss(vgg(content_img).detach()) for i in content_layers]\n",
    "    #style_losses = [StyleLoss(vgg(style_img).detach()) for i in style_layers]\n",
    "    \n",
    "    \n",
    "    #compute total loss and make result image\n",
    "    optimizer = optim.LBFGS([noise_img.requires_grad_()])# LBFGS is a kind of alogorithms.it makes computation more easier\n",
    "    noise_img = Variable(noise_img, requires_grad = True)\n",
    "    makeNewImg(noise_img, vgg, content_layers_target, style_layers_target, input_img_content, input_img_style, content_layers,  style_layers);\n",
    "    \n",
    "    #plt.figure()\n",
    "    #showImage(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()# call sueprclass' init method\n",
    "        self.target = target.detach()\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        print(self.loss)\n",
    "        return input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gramMatrix(target).detach()\n",
    "    def forward(self, input):\n",
    "        gram_input = gramMatrix(input)\n",
    "        self.loss = F.mse_loss(gram_input, self.target)\n",
    "        print(self.loss)\n",
    "        return input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOutput(vgg, input_img, style_losses, content_losses):\n",
    "    num = 1\n",
    "    print(content_losses)\n",
    "    print(style_losses)\n",
    "    while num <= STEPS:\n",
    "        \n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)##\n",
    "            vgg(input_img)\n",
    "            optimizer.zero_grad()\n",
    "            style_total = 0\n",
    "            content_total = 0\n",
    "            for i in content_losses:\n",
    "                content_total += i.loss\n",
    "            for i in style_losses:\n",
    "                style_total += i.loss\n",
    "            content_total = content_total * CNTWEIGHT\n",
    "            style_total  = style_total * STLWEIGHT\n",
    "            total_loss = style_total + content_total \n",
    "            total_loss.backward()\n",
    "            if num == 100:\n",
    "                print(\"run {}:\".format(num))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(style_total.item(), content_total.item()))\n",
    "                print()\n",
    "            return style_total + content_total\n",
    "                \n",
    "        optimizer.step(closure)\n",
    "        num  = num+1\n",
    "    return input_img    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
